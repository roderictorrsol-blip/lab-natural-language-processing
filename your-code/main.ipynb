{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(r\"C:\\Users\\Usuario\\Desktop\\IA\\CURSO\\WEEK_4\\DAY_2\\EXERCICES\\lab-natural-language-processing\\data\\kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                           Will do.      0\n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                fyi      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data ['text']\n",
    "y = data ['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label\n",
      "0    DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                             Will do.      0\n",
      "2    Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3    Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                  fyi      0\n",
      "..                                                 ...    ...\n",
      "995  So what's the latest? It sounds contradictory ...      0\n",
      "996  TRANSFER OF 36,759,000.00 MILLION POUNDS TO YO...      1\n",
      "997  Barb I will call to explain. Are you back in t...      0\n",
      "998    Yang on travelNot free tonite.May work tomorrow      0\n",
      "999  sbwhoeopSunday February 21 2010 7:42 PMHShaunH...      0\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "\n",
    "def clean_html(text):\n",
    "    # remove scripts and styles\n",
    "    text = re.sub(r'<(script|style).*?>.*?</\\1>', '', text, flags=re.S)\n",
    "\n",
    "    # remove comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.S)\n",
    "\n",
    "    # remove all remaining tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "data ['text'] = data['text'].apply(clean_html)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label\n",
      "0    dear sir strictly private business proposal am...      1\n",
      "1                                              will do      0\n",
      "2    nora cheryl has emailed dozens of memos about ...      0\n",
      "3    dear sir fmadam know that this proposal might ...      1\n",
      "4                                                  fyi      0\n",
      "..                                                 ...    ...\n",
      "995  so what the latest it sounds contradictory and...      0\n",
      "996  transfer of million pounds to youraccount my n...      1\n",
      "997  barb will call to explain are you back in the ...      0\n",
      "998    yang on travelnot free tonite may work tomorrow      0\n",
      "999  sbwhoeopsunday february pmhshaunh just talked ...      0\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def clean_text_2(text):\n",
    "    # Remove prefixed b\n",
    "    text = re.sub(r\"^b\\s+\", \"\", text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "\n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', ' ', text)\n",
    "\n",
    "    # Remove single characters from start\n",
    "    text = re.sub(r'^\\s*[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "data ['text'] = data['text'].apply(clean_text_2)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label\n",
      "0    dear sir strictly private business proposal mi...      1\n",
      "1                                                           0\n",
      "2    nora cheryl emailed dozens memos haiti weekend...      0\n",
      "3    dear sir fmadam know proposal might surprise e...      1\n",
      "4                                                  fyi      0\n",
      "..                                                 ...    ...\n",
      "995  latest sounds contradictory af decide shall ta...      0\n",
      "996  transfer million pounds youraccount name mr ej...      1\n",
      "997                     barb call explain back country      0\n",
      "998       yang travelnot free tonite may work tomorrow      0\n",
      "999  sbwhoeopsunday february pmhshaunh talked shaun...      0\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "data ['text'] = data['text'].apply(remove_stopwords)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label\n",
      "0    dear sir strictly private business proposal mi...      1\n",
      "1                                                           0\n",
      "2    nora cheryl emailed dozen memo haiti weekend p...      0\n",
      "3    dear sir fmadam know proposal might surprise e...      1\n",
      "4                                                  fyi      0\n",
      "..                                                 ...    ...\n",
      "995  latest sound contradictory af decide shall tak...      0\n",
      "996  transfer million pound youraccount name mr eji...      1\n",
      "997                     barb call explain back country      0\n",
      "998       yang travelnot free tonite may work tomorrow      0\n",
      "999  sbwhoeopsunday february pmhshaunh talked shaun...      0\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "data ['text'] = data['text'].apply(lemmatize_text)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in HAM:\n",
      "           word  count\n",
      "5416      state    136\n",
      "4302         pm    127\n",
      "6463      would    107\n",
      "4433  president     99\n",
      "5855       time     95\n",
      "883        call     94\n",
      "3688         mr     91\n",
      "3902      obama     84\n",
      "4219    percent     81\n",
      "5058  secretary     79\n",
      "\n",
      "Top 10 words in SPAM:\n",
      "              word  count\n",
      "13872        money    981\n",
      "230        account    895\n",
      "2403          bank    800\n",
      "7931          fund    781\n",
      "21155  transaction    549\n",
      "3170      business    514\n",
      "4481       country    508\n",
      "13993           mr    489\n",
      "14369         nbsp    475\n",
      "13682      million    460\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "ham = data[data['label'] == 0]['text']\n",
    "spam = data[data['label'] == 1]['text']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "def get_top_words(text_series, n=10):\n",
    "    X = vectorizer.fit_transform(text_series)\n",
    "    word_counts = X.toarray().sum(axis=0)\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    freq_df = pd.DataFrame({\n",
    "        'word': words,\n",
    "        'count': word_counts\n",
    "    })\n",
    "    \n",
    "    return freq_df.sort_values(by='count', ascending=False).head(n)\n",
    "\n",
    "top_ham = get_top_words(ham, 10)\n",
    "top_spam = get_top_words(spam, 10)\n",
    "\n",
    "print(\"Top 10 words in HAM:\")\n",
    "print(top_ham)\n",
    "\n",
    "print(\"\\nTop 10 words in SPAM:\")\n",
    "print(top_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>regard mr nelson smith kindly reply private em...</td>\n",
       "      <td>1</td>\n",
       "      <td>regard mr nelson smith kindly reply private em...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "      <td>0</td>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>huma abedin checking pat work jack jake rest a...</td>\n",
       "      <td>0</td>\n",
       "      <td>huma abedin checking pat work jack jake rest a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>announced monday today</td>\n",
       "      <td>0</td>\n",
       "      <td>announced monday today</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "      <td>1</td>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "29   regard mr nelson smith kindly reply private em...      1   \n",
       "535         able reach oscar supposed send pdb receive      0   \n",
       "695  huma abedin checking pat work jack jake rest a...      0   \n",
       "557                             announced monday today      0   \n",
       "836  bank africaagence san pedro bp san pedro cote ...      1   \n",
       "\n",
       "                                     preprocessed_text  money_mark  \\\n",
       "29   regard mr nelson smith kindly reply private em...           0   \n",
       "535         able reach oscar supposed send pdb receive           0   \n",
       "695  huma abedin checking pat work jack jake rest a...           0   \n",
       "557                             announced monday today           0   \n",
       "836  bank africaagence san pedro bp san pedro cote ...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "29                  0        79  \n",
       "535                 0        42  \n",
       "695                 0        76  \n",
       "557                 0        22  \n",
       "836                 1      1050  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "data_train['preprocessed_text'] = data_train['text'].apply(lemmatize_text).astype(str)\n",
    "data_val['preprocessed_text']   = data_val['text'].apply(lemmatize_text).astype(str)\n",
    "\n",
    "\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 27977)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train_bow = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "\n",
    "X_val_bow = vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "print(X_train_bow.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 29742)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "[[122   3]\n",
      " [  7  68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       125\n",
      "           1       0.96      0.91      0.93        75\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.95      0.94      0.95       200\n",
      "weighted avg       0.95      0.95      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(data_train[\"preprocessed_text\"])\n",
    "X_val_tfidf   = tfidf.transform(data_val[\"preprocessed_text\"])\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "extra_cols = [\"money_mark\", \"suspicious_words\", \"text_len\"]\n",
    "\n",
    "X_train_extra = data_train[extra_cols].values\n",
    "X_val_extra   = data_val[extra_cols].values\n",
    "\n",
    "# Combine sparse TF-IDF and dense extra features\n",
    "X_train_all = hstack([X_train_tfidf, X_train_extra])\n",
    "X_val_all   = hstack([X_val_tfidf, X_val_extra])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_all, data_train[\"label\"])\n",
    "\n",
    "y_pred = clf.predict(X_val_all)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(data_val[\"label\"], y_pred))\n",
    "print(confusion_matrix(data_val[\"label\"], y_pred))\n",
    "print(classification_report(data_val[\"label\"], y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
